{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "hrONjaZqj5I0",
        "outputId": "4b397d58-b735-4346-a3c2-88946ddc5a66"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sagemaker'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-14949db425d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuggingface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"arn:aws:iam::123456789012:role/SageMakerExecutionRole\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sagemaker\n",
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "sess = sagemaker.Session()\n",
        "role = \"arn:aws:iam::123456789012:role/SageMakerExecutionRole\"\n",
        "\n",
        "huggingface_model = HuggingFaceModel(\n",
        "    model_data=\"s3://your-bucket/distilgpt2-sdsu.tar.gz\",\n",
        "    role=role,\n",
        "    transformers_version=\"4.26.0\",\n",
        "    pytorch_version=\"1.13.1\",\n",
        "    py_version=\"py39\",\n",
        "    entry_point=\"inference.py\"  # defines model_fn and predict_fn\n",
        ")\n",
        "\n",
        "predictor = huggingface_model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type=\"ml.m5.large\",\n",
        "    endpoint_name=\"AdmissionsAgentEndpoint\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "def model_fn(model_dir):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    return {\"model\": model, \"tokenizer\": tokenizer}\n",
        "\n",
        "def predict_fn(data, model_artifacts):\n",
        "    input_text = data[\"inputs\"]\n",
        "    inputs = model_artifacts[\"tokenizer\"](input_text, return_tensors=\"pt\")\n",
        "    outputs = model_artifacts[\"model\"].generate(**inputs, max_new_tokens=50)\n",
        "    return model_artifacts[\"tokenizer\"].decode(outputs[0])\n",
        ""
      ],
      "metadata": {
        "id": "MT-CqCxEkOJa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}